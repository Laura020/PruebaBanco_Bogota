import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
class TransformarDatos(beam.DoFn):
def process(self, row):
values = row.split(',')
if len(values) < 4:
return # Omitir filas incompletas
id_transaccion, monto, fecha, cliente_id = values
try:
monto = float(monto) # Convertir el monto a número
fecha = fecha.strip() # Asegurar formato correcto de fecha
return [(id_transaccion, monto, fecha, cliente_id)]
except:
return # Omitir datos corruptos
options = PipelineOptions(
runner='DataflowRunner',
project='[TU_PROYECTO_GCP]',
temp_location='gs://data-lake-transacciones/temp'
)
with beam.Pipeline(options=options) as p:
(
p | 'Leer Archivo' >> beam.io.ReadFromText('gs://data-lake-transacciones/datos_transacciones.csv')
| 'Transformar' >> beam.ParDo(TransformarDatos())
| 'Guardar Limpio' >> beam.io.WriteToText('gs://data-lake-transacciones/procesado/datos_limpios.csv')
)import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
class TransformarDatos(beam.DoFn):
def process(self, row):
values = row.split(',')
if len(values) < 4:
return # Omitir filas incompletas
id_transaccion, monto, fecha, cliente_id = values
try:
monto = float(monto) # Convertir el monto a número
fecha = fecha.strip() # Asegurar formato correcto de fecha
return [(id_transaccion, monto, fecha, cliente_id)]
except:
return # Omitir datos corruptos
options = PipelineOptions(
runner='DataflowRunner',
project='[TU_PROYECTO_GCP]',
temp_location='gs://data-lake-transacciones/temp'
)
with beam.Pipeline(options=options) as p:
(
p | 'Leer Archivo' >> beam.io.ReadFromText('gs://data-lake-transacciones/datos_transacciones.csv')
| 'Transformar' >> beam.ParDo(TransformarDatos())
| 'Guardar Limpio' >> beam.io.WriteToText('gs://data-lake-transacciones/procesado/datos_limpios.csv')
clear
nano dataflow_transformation.py
exit()
